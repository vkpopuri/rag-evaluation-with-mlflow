{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a84fc6-6cb0-4aa5-bc53-994f3a51c275",
   "metadata": {},
   "source": [
    "## Introduction to LLM & RAG evaluation with MLflow - Example Notebook\n",
    "### Overview\n",
    "#### This repository contains sample code for evaluating RAG systems using Mlflow metrics, relevance and latency. Please note that the code provided here is for illustrative purposes only and is not for production deployment. Refer to the documentation links included in the readme.md for additional details on Mlflow services and metrics\n",
    "\n",
    "#### Services and libraries were issued to illustrate the solution.\n",
    "\n",
    "##### - Azure Open AI: gpt-4 is used for embedding and completions.\n",
    "##### - Open AI: Mlflow uses Open AI for evaluation\n",
    "##### - Chroma and Qdrant: Vector databases for storing embedded domain specific documents.\n",
    "##### - Mlflow AI Gateway(Experimental): A central configuration management service deployed      locally. LLM provider, endpoint and API keys can be stored using Mlflow API gateway.\n",
    "##### - Mlflow Tracking server: MLflow tracking server is a stand-alone HTTP server that serves multiple REST API endpoints for tracking runs/experiments.\n",
    "##### - Langchain\n",
    "##### - Sample Mlflow documentation for QnA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741deab-e5dd-4a98-95b0-60a025c0ab19",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Complete all the steps mentioned in the prerequiste section of the readme.md in the repository before running the cells in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0faced5-4cd3-4271-b7ee-84dfbd91fa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required libraries and packages inluded in the requirements.txt file\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad3421-2e11-413d-b410-3f89fdcc95be",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492e88b-f42f-47a5-8211-7ac773d927d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import langchain \n",
    "from openai import AzureOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.deployments\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import Mlflow\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from mlflow.metrics.genai import relevance\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.embeddings import MlflowEmbeddings\n",
    "import qdrant_client\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import CollectionStatus\n",
    "from langchain.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "import langchain_qdrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.self_query.qdrant import QdrantTranslator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68028ee3-aa76-4dad-bd92-50b75126bba2",
   "metadata": {},
   "source": [
    "### Retrieve environment variables and set tracking URI, LLMs for embedding and Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d123677-f9bb-4828-a2fd-7d91c8b789ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')\n",
    "AZURE_OPENAI_ENDPOINT=os.environ.get('AZURE_OPENAI_ENDPOINT')\n",
    "tracking_uri = \"http://localhost:8080/\"                               \n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "llm = Mlflow(target_uri=\"http://127.0.0.1:5000\", endpoint=\"completions\")\n",
    "ebedd_llm = Mlflow(target_uri=\"http://127.0.0.1:5000\", endpoint=\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe578b-9a2c-4df6-9009-6f703644c77c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Mlflow sample documents,embed and ingest the loaded documents into Chroma vector database and instantiate a retriever \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4faefd-4ab6-4da9-9667-9180693ef026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Mlflow sample documents using Langchain WebBaseLoader and create a Retriever for querying Chroma vector store\n",
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://mlflow.org/docs/latest/index.html\",\n",
    "        \"https://mlflow.org/docs/latest/tracking/autolog.html\",\n",
    "        \"https://mlflow.org/docs/latest/deep-learning/index.html\",\n",
    "        \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\n",
    "        \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\",\n",
    "        \"https://mlflow.org/docs/latest/tracking/autolog.html\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "CHUNK_SIZE = 1000\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = MlflowEmbeddings(\n",
    "    target_uri=\"http://127.0.0.1:5000\",\n",
    "    endpoint=\"embeddings\",\n",
    ")\n",
    "\n",
    "docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(fetch_k=3),\n",
    "    return_source_documents=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2c83a-c519-4ad4-b07d-e69bdee6d37b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe to store a list of questions to evaluate on.\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is deep learning?\",\n",
    "            \"How to monitor models using mlflow?\",\n",
    "            \"Explain Mlflow auto logging feature\",\n",
    "            \"What is Deep learning\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebede5-663f-4972-9e5a-b19294f66af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insitialize an in-memory Qdrant store, metadata for retriever for Langchain SelfQueryRetriver for querying Qdrant vector database\n",
    "\n",
    "Qdrant_store = Qdrant.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"MLflow Collection\",)\n",
    "#source, title,language _id,_collection_name\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.self_query.qdrant import QdrantTranslator\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Mlflow documentation link\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"Title of the web page\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"language\",\n",
    "        description=\"Language of mlflow documentation\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "   \n",
    "    \n",
    "]\n",
    "\n",
    "document_content_description = \"Mlflow documentation repository\"\n",
    "#llm = OpenAI(temperature=0)\n",
    "qdrant_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, Qdrant_store, document_content_description, metadata_field_info, structured_query_translator=QdrantTranslator(metadata_key=\"metadata\"),verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ca1a7-1f52-4f89-9977-df429c87f6cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create necessary functions to apply questions to Chroma and Qdrant retrievers for evaluation\n",
    "\n",
    "def chroma_model(input_df):\n",
    "    return input_df[\"questions\"].map(qa).tolist()\n",
    "\n",
    "def qdrant_model(input_df):\n",
    "    #return input_df['questions'].map(qdrant.similarity_search).tolist()\n",
    "    return input_df['questions'].map(qdrant_retriever.invoke)\n",
    "\n",
    "def mlflow_evaluate (vectordb,input_df):\n",
    "    relevance_metric = relevance()\n",
    "    if \"chroma\" in vectordb.lower():\n",
    "        with mlflow.start_run():\n",
    "    \n",
    "            Chroma_results = mlflow.evaluate(\n",
    "                chroma_model,\n",
    "                data=eval_df,\n",
    "            #model_type=\"question-answering\", \n",
    "                evaluators=\"default\",\n",
    "                predictions=\"result\",\n",
    "                extra_metrics=[relevance_metric, mlflow.metrics.latency()],\n",
    "                evaluator_config={\n",
    "                    \"col_mapping\": {\n",
    "                        \"inputs\": \"questions\",\n",
    "                        \"context\": \"result\"\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        print(Chroma_results.metrics)\n",
    "        display(Chroma_results.tables[\"eval_results_table\"])\n",
    "        print(f\"See aggregated evaluation results below: \\n\\n\")\n",
    "        df_c= pd.DataFrame(Chroma_results.metrics,[0])\n",
    "        return df_c\n",
    "\n",
    "    elif 'qdrant' in vectordb.lower():\n",
    "        with mlflow.start_run():\n",
    "    \n",
    "            Qdrant_results = mlflow.evaluate(\n",
    "                 qdrant_model,\n",
    "                data=eval_df,\n",
    "            #model_type=\"question-answering\", \n",
    "                evaluators=\"default\",\n",
    "                predictions=\"result\",\n",
    "                extra_metrics=[relevance_metric, mlflow.metrics.latency()],\n",
    "                evaluator_config={\n",
    "                    \"col_mapping\": {\n",
    "                        \"inputs\": \"questions\",\n",
    "                        \"context\": \"result\"\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    print(Qdrant_results.metrics)\n",
    "    display(Qdrant_results.tables[\"eval_results_table\"])\n",
    "    print(f\"See aggregated evaluation results below: \\n\\n\")\n",
    "    df_q= pd.DataFrame(Qdrant_results.metrics,[0])\n",
    "    return df_q\n",
    "\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7449cf-4adb-4f8a-b242-925e840e5c9d",
   "metadata": {},
   "source": [
    "### Experiment #1: Evaluate performance of the RAG system with Chroma Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67101c-6eec-416d-a173-55a9a1499811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function created above to process questions using Chroma Vector Database\n",
    "df_c= mlflow_evaluate('Chroma',eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63faa70-0add-4bfc-8857-b232409e5a67",
   "metadata": {},
   "source": [
    "### Experiment #2: Evaluate RAG system's relevance and latency with Qdrant Vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfe099-a0ae-42f2-96c6-d8151564dd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function to query Qdrant in-memory store\n",
    "df_q= mlflow_evaluate('Qdrant',eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb5584-1b81-4543-8dec-43c752ff9671",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Now let us plot the relevance and latency values from experiment 1 and 2 for comparion purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52755ea-76ab-4881-9e22-72cc376cd001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "c_values= df_c[['latency/mean', 'relevance/v1/mean', 'relevance/v1/p90']].squeeze()\n",
    "q_values= df_q[['latency/mean', 'relevance/v1/mean', 'relevance/v1/p90']].squeeze()          \n",
    "\n",
    "labels = ['latency/mean', 'relevance/v1/mean', 'relevance/v1/p90']\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, c_values, width, label='Chroma')\n",
    "rects2 = ax.bar(x + width/2, q_values, width, label='Qdrant')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Comparison of relevance and latency metrics between Chroma and Qdrant')\n",
    "ax.set_xticks(x, labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407eb93-fd2f-45c9-a8be-805de04d2c3f",
   "metadata": {},
   "source": [
    "#### Note: The above comparison is for illustration purpose only,and is not to convery one vector database is better than the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showcode": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
